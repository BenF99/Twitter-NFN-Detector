# Twitter-NFN-Detector

**A Final Year Project at university that provides a system to detecting neural fake news/machine-generated Text on Twitter.**

<!-- TABLE OF CONTENTS -->
<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
      <ul>
        <li><a href="#built-with">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#contact">Contact</a></li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project
<p align="center">
  <img width="580" height="500" src="https://i.gyazo.com/ec12d55f1cf276a85c3286ca521e92fc.png">
</p>

**Dissertation Abstract**

Neural Fake News (NFN), defined as fictious information masquerading as legitimate news that has been generated by a neural network-based language model, can be a critical societal issue. In recent years, unsupervised generative language models such as GPT-2 have proven to generate extremely coherent paragraphs of text. These systems enable malicious actors to scale up their operations by delivering automatically generated disinformation across social media. Developing defence mechanisms against NFN is critical in preventing sites such as Twitter falling victim to an upsurge in the spread of synthetic text. I thus present a system that detects machine generated text that is broadcasted on Twitter, utilising fine-tuned pre-trained language models trained on the classification of outputs released with GPT-2. My system applies the weights released with the OpenAI detector model and two fine-tuned models: DeBERTa and XLNet, to classify real (human-written) and fake (machine-generated) text. I find that DeBERTa achieves a 96% classification accuracy with limited resources, competing with the OpenAI detector model that achieved ~95% across three sampling methods. I argue that with the access to more powerful hardware capable of processing large sequence lengths, fine-tuning DeBERTa will likely outperform OpenAIâ€™s detector. I also investigate the presence of machine-generated tweets on Twitter and find that they are not currently ubiquitous on social media. I conclude by discussing the importance of research into the detection of machine generated content and suggest that social media platforms implement classification systems as natural language generative models popularise. 

### Built With

* [Transformers](https://github.com/huggingface/transformers)
* [SimpleTransformers](https://github.com/ThilinaRajapakse/simpletransformers)
* [Anvil.Works](https://anvil.works/)
* [Python-Twitter-Tools](https://github.com/python-twitter-tools/twitter)
* [Firebase](https://firebase.google.com/)
* [OpenAI Detector](https://github.com/openai/gpt-2-output-dataset/tree/master/detector)

<!-- GETTING STARTED -->
## Getting Started


### Prerequisites

1) Install fine-tuned models:

[OpenAI RoBERTa Detector](https://github.com/openai/gpt-2-output-dataset/tree/master/detector): 
   ```sh
   https://github.com/openai/gpt-2-output-dataset/tree/master/detector
   ```
Fine-tuned `DeBERTa-large`:
   ```sh
   -
   ```
Fine-tuned `XLNet-large-cased`:
   ```sh
   -
   ```
### Installation


<!-- USAGE EXAMPLES -->
## Usage

<!-- CONTACT -->
## Contact


<!-- ACKNOWLEDGEMENTS -->
## Acknowledgements

<!-- MARKDOWN LINKS AND IMAGES -->
[detector-screenshot]: "https://i.gyazo.com/ec12d55f1cf276a85c3286ca521e92fc.png"
